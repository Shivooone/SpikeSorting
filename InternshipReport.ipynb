{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401335a4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This notebook will cover the SpikeInterface pipeline for extracellular analysis and spike sorting comparison. \n",
    "\n",
    "Spikeinterface wraps 5 subpackages: spikeextractors, spikesorters, spiketoolkit, spikecomparison, and spikewidgets.\n",
    "\n",
    "For this analysis, we will use our 384-channel dataset from Neuropixels obtained during the PVT-Manual slow paradigm experimentation with vestibular stimulation by rotation of the stage.\n",
    "\n",
    "The summary of the pipeline is the following :\n",
    "\n",
    "- load the data with spikeextractors package\n",
    "- load the probe information using ProbeInterface\n",
    "- preprocess the signals\n",
    "- run a popular spike sorting algorithm with different parameters\n",
    "- curate the spike sorting output using 1) quality metrics (automatic) - 2) Phy (manual) - 3) consensus-based\n",
    "\n",
    "All these operations are executed in a new Python 3.7 Anaconda environment named \"Spikeline\" specifically created for the data analysis purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55724363",
   "metadata": {},
   "source": [
    "# Importing the modules\n",
    "\n",
    "Let's now import the `spikeinterface` modules that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c858c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:09:40 [I] klustakwik KlustaKwik2 version 0.2.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os \n",
    "import spikeinterface\n",
    "import spikeinterface.extractors as se \n",
    "import spikeinterface.toolkit as st\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.widgets as sw\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "from varname import nameof\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ed213",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Loading recording and probe information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf18828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\003WO.1_007_PVTmanual_slowslow_g0_t0.imec0.ap.bin\n"
     ]
    }
   ],
   "source": [
    "# Add the path to the folder containing the data folder where are stored \n",
    "\n",
    "\n",
    "Path_to_folder_containing_data = r'F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0'\n",
    "\n",
    "Path = r'F:\\Data_recording\\003WO.1_014_PVT_2_103_g0_imec0'\n",
    "# Function to read the file with the dedicated SpikeExtractor for the output of our recording software SpikeGLX, the function does a loop in case other recording folders exist but here we have only one analysis\n",
    "def Read_file(Path_to_folder_containing_data):\n",
    "    for fichier in os.listdir(Path_to_folder_containing_data):        \n",
    "        if fichier.endswith('.imec0.ap.bin'):\n",
    "            recording = se.SpikeGLXRecordingExtractor(file_path=os.path.join(Path_to_folder_containing_data,fichier))\n",
    "            file_path = os.path.join(Path_to_folder_containing_data,fichier)\n",
    "            print(file_path)\n",
    "            return recording\n",
    "        \n",
    "recording = Read_file(Path_to_folder_containing_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9e13f",
   "metadata": {},
   "source": [
    "## Exploring the data before pre-processing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b804948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Recording traces')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Time(s)')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SpikeWidgets is a package of the SpikeInterface project that was designed for visualization of various aspects of the spike sorting pipeline. \n",
    "#The widgets include: raster plots, autocorrelograms, crosscorrelograms, isi distributions, and more.\n",
    "#We look at the data before pre-processing stage\n",
    "\n",
    "\n",
    "range1 = [range(48,52)]\n",
    "range2 = [*range(91,96)]\n",
    "range3 = [range(59,65)]\n",
    "#Visualizing the recording extractor object\n",
    "w_ts = sw.plot_timeseries(recording, channel_ids=range2)\n",
    "plt.title('Recording traces')\n",
    "plt.xlabel('Time(s)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96df73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SpikeToolkit is a package of the SpikeInterface project is designed for efficient preprocessing, postprocessing, evaluation, and curation of extracellular datasets and spike sorting outputs.\n",
    "#We can now pre-process uniformly the recording for the spikesorters\n",
    "\n",
    "\n",
    "def pre_processing(recording):\n",
    "    #We remove the ground channel from the recording\n",
    "    recording1 = st.preprocessing.remove_bad_channels(recording, bad_channel_ids=[191])\n",
    "    #Filtering to separate them: high pass filtering for the spikes (generally set at 6000Hz) and low-pass filtering for the synaptic mechanisms (generally set at 300Hz).  \n",
    "    recording2 = st.preprocessing.bandpass_filter(recording1, freq_min=300, freq_max=6000)\n",
    "    #Whiten the signal\n",
    "    recording3 = st.preprocessing.whiten(recording2)\n",
    "    #Application of common median reference, the median of the selected channels is removed for each timestamp\n",
    "    recording4 = st.preprocessing.common_reference(recording3, reference='median')\n",
    "    return recording4\n",
    "\n",
    "\n",
    "recording_preprocessed = pre_processing(recording=recording)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd828056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#All operations in SpikeInterface are lazy, meaning that they are not performed if not needed. This is why the creation of our filter recording was almost instantaneous. However, to speed up further processing, we might want to cache it to a file and perform those operations (eg. filters, CMR, etc.) at once. This is particularly important if we are going to extract waveforms, templates, pca scores, or in general post-process the results.\n",
    "#If we now closed the Python session, we would have a nice .dat file, but no information on how to open it! In order to save the state of an extractor, we can use the dumping mechanism. Each extractor can be converted to a dictionary, which holds the path to the data file and all relevant information:\n",
    "def dump_to_pkl(recording,Path_to_folder_containing_data):\n",
    "    recording_cache = se.CacheRecordingExtractor(recording, n_jobs=1, chunk_mb=500, verbose=True)\n",
    "    recording_cache.filename\n",
    "    recording_cache.get_tmp_folder()\n",
    "    recording_cache.move_to(Path_to_folder_containing_data+'/dataall.dat') \n",
    "    recording_cache.dump_to_dict()\n",
    "    pkl = Path_to_folder_containing_data+'/filtered_dataall_'+ os.path.basename(Path_to_folder_containing_data)+'.pkl'\n",
    "    recording_cache.dump_to_pickle(pkl)\n",
    "    return pkl\n",
    "\n",
    "#dump_to_pkl(recording=recording_preprocessed,Path_to_folder_containing_data=Path_to_folder_containing_data)\n",
    "\n",
    "#Loading the filtered data stored in pkl. In another session, we can pick up from where we left by loading the extractor from the pickle file:\n",
    "def load_recording(pkl_path):\n",
    "    recording_loaded = se.load_extractor_from_pickle(pkl_path)\n",
    "    return recording_loaded\n",
    "\n",
    "pkl_path=Path_to_folder_containing_data+'/filtered_dataall_'+ os.path.basename(Path_to_folder_containing_data)+'.pkl'\n",
    "recording_loaded = load_recording(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33324d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Recording traces')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Time(s)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Voltage')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-processed data loaded plot view\n",
    "\n",
    "w_ts = sw.plot_timeseries(recording_preprocessed, channel_ids=[209, 181, 26, 30])\n",
    "plt.title('Recording traces')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Voltage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f61ab",
   "metadata": {},
   "source": [
    "## Spike sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a85902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['combinato',\n",
       " 'hdsort',\n",
       " 'herdingspikes',\n",
       " 'ironclust',\n",
       " 'kilosort',\n",
       " 'kilosort2',\n",
       " 'kilosort2_5',\n",
       " 'kilosort3',\n",
       " 'klusta',\n",
       " 'mountainsort4',\n",
       " 'spykingcircus',\n",
       " 'tridesclous',\n",
       " 'waveclus',\n",
       " 'yass']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#We can now run spike sorting on the above recording. We decided to use 3 sorters in the purpose of our analysis, HerdingSpikes, Ironclust, Kilosort2 and Tridesclous\n",
    "#Functions are defined for each sorter\n",
    "#We can check that all the sorters we want to use are installed\n",
    "ss.available_sorters()\n",
    "\n",
    "#For MATLAB-based sorters - Ironclust and Kilosort2, we had to clone the sorter repo and point it to SpikeInterface\n",
    "def Run_Kilosort(recording_loaded,Path_results):\n",
    "    ss.Kilosort2Sorter.set_kilosort2_path(r'F:\\Sorters\\Kilosort2')\n",
    "    Path_kilosort_results = os.path.join(Path_results,'_KS2_results')\n",
    "    os.mkdir(Path_kilosort_results)\n",
    "    sorting_KS2 = ss.run_kilosort2(recording_loaded, car = 'False', output_folder=Path_kilosort_results)\n",
    "    return sorting_KS2, Path_kilosort_results\n",
    "\n",
    "def Run_Tridesclous(recording_loaded,Path_results):\n",
    "    Path_tridesclous_results = os.path.join(Path_results,'_TDC_results')\n",
    "    os.mkdir(Path_tridesclous_results)\n",
    "    sorting_TDC = ss.run_tridesclous(recording_loaded, detect_threshold = 6, output_folder=Path_tridesclous_results)\n",
    "    return sorting_TDC, Path_tridesclous_results\n",
    "\n",
    "def Run_Ironclust(recording_loaded,Path_results):\n",
    "    ss.IronClustSorter.set_ironclust_path(r'F:\\Sorters\\ironclust2')\n",
    "    Path_ironclust_results = os.path.join(Path_results,'_Ironclust_results')\n",
    "    os.mkdir(Path_ironclust_results)\n",
    "    sorting_IC = ss.run_ironclust(recording_loaded, output_folder=Path_ironclust_results, detect_threshold = 6, pc_per_chan = 3, common_ref_type = 'none')\n",
    "    return sorting_IC, Path_ironclust_results\n",
    "\n",
    "\n",
    "def Run_sorters(recording_loaded,Path_to_folder_containing_data):\n",
    "    Path_results=os.path.join(Path_to_folder_containing_data,'Sorting_results')\n",
    "    os.mkdir(Path_results)\n",
    "    Sorters = []\n",
    "    Pathsresults = []\n",
    "    try :\n",
    "        sorting_KS2, Path_kilosort_results = Run_Kilosort(recording_loaded,Path_results)\n",
    "        Sorters.append(sorting_KS2)\n",
    "        Pathsresults.append(Path_kilosort_results)\n",
    "    finally:\n",
    "           try :\n",
    "                sorting_TDC, Path_tridesclous_results = Run_Tridesclous(recording_loaded=recording_loaded,Path_results=Path_results)\n",
    "                Sorters.append(sorting_TDC)\n",
    "                Pathsresults.append(Path_tridesclous_results)     \n",
    "           finally:\n",
    "                  try :\n",
    "                      sorting_IC, Path_ironclust_results = Run_Ironclust(recording_loaded=recording_loaded,Path_results=Path_results)\n",
    "                      Sorters.append(sorting_IC)\n",
    "                      Pathsresults.append(Path_ironclust_results)\n",
    "                  finally:\n",
    "                      return Sorters, Pathsresults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dec4b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatsource is not found {'filenames': ['D:\\\\Data_meeting_Flavia\\\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\\\dataall.dat'], 'dtype': '<f4', 'sample_rate': 30000.168908, 'total_channel': 384, 'offset': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Path_results = r'F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\Sorting_results'\n",
    "\n",
    "#To perform manual curation later we will export the data to Phy.\n",
    "def Export_to_Phy(sorting, result_paths, recording_loaded):\n",
    "    for i in range(0,len(sorting)):\n",
    "        try :\n",
    "            Path_phy_result = os.path.join(result_paths[i], 'Phy')\n",
    "            st.postprocessing.export_to_phy(recording_loaded, \n",
    "                                    sorting[i], output_folder=Path_phy_result,verbose=True, recompute_info=True, n_jobs=1,max_channels_per_template=10)\n",
    "        finally :\n",
    "            pass\n",
    "\n",
    "Path_kilosort_results= r'F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\Sorting_results\\_KS2_results'\n",
    "Path_tridesclous_results =r'F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\Sorting_results\\_TDC_results'\n",
    "Path_ironclust_results =r'F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\Sorting_results\\_Ironclust_results\\tmp\\firings.mda'\n",
    "\n",
    "#The sorting extractors allow to reload easily the sorting results obtained\n",
    "sorting_KS2 = se.KiloSortSortingExtractor(Path_kilosort_results)\n",
    "sorting_TDC = se.TridesclousSortingExtractor(Path_tridesclous_results)\n",
    "sorting_IC = se.MdaSortingExtractor(Path_ironclust_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed00684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The spike sorting returns a SortingExtractor object. Let's see show many units Kilosort2 found\n",
    "print('Units', sorting_KS2.get_unit_ids())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350ebe0",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d839a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Raster plot')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Time(s)')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Unit id')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(300, 384, 180)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#The postprocessing submodule of spiketoolkit allow us to extract information from the combination of the recording and sorting extractors. For example, we can extract waveforms, templates, maximum channels and pca scores.\n",
    "#It allows to post-process the data directly through the API before exporting it to Phy\n",
    "#We can visualize the raster plot of the spiking activity of a group of neurons over time. In a raster plot each row (y-axis) corresponds to the index of a neuron in a neuron group. The columns (x-axis) corresponds to the current time in the simulation. \n",
    "#The presence of a dot in a given row and column, indicates that the neuron whose index corresponds to that row produced an action potential (spike) at the time corresponding to that column. For instance, if neuron 2 spikes at time 10 a dot will appear in row 2 at the column representing the 10th time index. \n",
    "#Extending this it can be seen that a raster plot displays the pattern of spikes across a neuron group over time.\n",
    "\n",
    "w_rs = sw.plot_rasters(sorting_KS2, trange=[0,10])\n",
    "plt.title('Raster plot')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Unit id')\n",
    "\n",
    "#We can see that the unit 78 presents a lot of activity along time, we can take a further look at it by looking at the waveforms and templates extracted\n",
    "#Each waveform is associated with a specific spike, so they are saved as spike features\n",
    "\n",
    "waveforms = st.postprocessing.get_unit_waveforms(recording_loaded, sorting_KS2)\n",
    "waveforms[78].shape\n",
    "len(sorting_KS2.get_unit_spike_train(78))\n",
    "\n",
    "#100 waveforms were extracted from the 1820 spike train of the first unit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dceae46",
   "metadata": {},
   "source": [
    "## Extra-cellular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cddafc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peak_to_valley',\n",
       " 'halfwidth',\n",
       " 'peak_trough_ratio',\n",
       " 'repolarization_slope',\n",
       " 'recovery_slope']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flavi\\anaconda3\\envs\\Spikeline\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:160: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "C:\\Users\\flavi\\anaconda3\\envs\\Spikeline\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:174: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "C:\\Users\\flavi\\anaconda3\\envs\\Spikeline\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:176: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_key</th>\n",
       "      <th>id_cluster</th>\n",
       "      <th>peak_to_valley</th>\n",
       "      <th>halfwidth</th>\n",
       "      <th>peak_trough_ratio</th>\n",
       "      <th>repolarization_slope</th>\n",
       "      <th>recovery_slope</th>\n",
       "      <th>Sorter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0sorting_KS2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0029666499636362646</td>\n",
       "      <td>0.0056333016163430194</td>\n",
       "      <td>-0.43529412150382996</td>\n",
       "      <td>1034604.0393495536</td>\n",
       "      <td>-136425.28109340102</td>\n",
       "      <td>sorting_KS2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1sorting_KS2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00383331175076596</td>\n",
       "      <td>0.006033299364249033</td>\n",
       "      <td>0.2704918086528778</td>\n",
       "      <td>nan</td>\n",
       "      <td>-47073.114058666804</td>\n",
       "      <td>sorting_KS2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2sorting_KS2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00016666572829417218</td>\n",
       "      <td>0.0051999707227781715</td>\n",
       "      <td>-1.6644736528396606</td>\n",
       "      <td>3849631.049327324</td>\n",
       "      <td>-777322.7612500505</td>\n",
       "      <td>sorting_KS2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3sorting_KS2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0003666646022471788</td>\n",
       "      <td>nan</td>\n",
       "      <td>6.08695650100708</td>\n",
       "      <td>nan</td>\n",
       "      <td>-34836.84386688921</td>\n",
       "      <td>sorting_KS2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4sorting_KS2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0009333280784473641</td>\n",
       "      <td>0.005266637014095841</td>\n",
       "      <td>-1.7006802558898926</td>\n",
       "      <td>3445331.8980281185</td>\n",
       "      <td>-3997790.365641963</td>\n",
       "      <td>sorting_KS2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4sorting_IC</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00013333258263533774</td>\n",
       "      <td>0.00013333258263533774</td>\n",
       "      <td>-0.6016949415206909</td>\n",
       "      <td>nan</td>\n",
       "      <td>-190575.3424644765</td>\n",
       "      <td>sorting_IC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5sorting_IC</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00013333258263533774</td>\n",
       "      <td>0.00013333258263533774</td>\n",
       "      <td>-0.36538460850715637</td>\n",
       "      <td>3093767.41863749</td>\n",
       "      <td>-168020.42651396102</td>\n",
       "      <td>sorting_IC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6sorting_IC</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0003999977479060132</td>\n",
       "      <td>0.006033299364249033</td>\n",
       "      <td>0.3083333373069763</td>\n",
       "      <td>nan</td>\n",
       "      <td>-40818.00578898942</td>\n",
       "      <td>sorting_IC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7sorting_IC</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00016666572829417218</td>\n",
       "      <td>0.00013333258263533774</td>\n",
       "      <td>-0.7423312664031982</td>\n",
       "      <td>2425794.9077953044</td>\n",
       "      <td>-176238.8169412743</td>\n",
       "      <td>sorting_IC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8sorting_IC</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00033333145658834436</td>\n",
       "      <td>0.004266642644330808</td>\n",
       "      <td>-0.699999988079071</td>\n",
       "      <td>1101150.1729484948</td>\n",
       "      <td>-280840.66399113426</td>\n",
       "      <td>sorting_IC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id_key id_cluster          peak_to_valley               halfwidth  \\\n",
       "0   0sorting_KS2          0   0.0029666499636362646   0.0056333016163430194   \n",
       "1   1sorting_KS2          1     0.00383331175076596    0.006033299364249033   \n",
       "2   2sorting_KS2          2  0.00016666572829417218   0.0051999707227781715   \n",
       "3   3sorting_KS2          3   0.0003666646022471788                     nan   \n",
       "4   4sorting_KS2          4   0.0009333280784473641    0.005266637014095841   \n",
       "..           ...        ...                     ...                     ...   \n",
       "4    4sorting_IC          4  0.00013333258263533774  0.00013333258263533774   \n",
       "5    5sorting_IC          5  0.00013333258263533774  0.00013333258263533774   \n",
       "6    6sorting_IC          6   0.0003999977479060132    0.006033299364249033   \n",
       "7    7sorting_IC          7  0.00016666572829417218  0.00013333258263533774   \n",
       "8    8sorting_IC          8  0.00033333145658834436    0.004266642644330808   \n",
       "\n",
       "       peak_trough_ratio repolarization_slope       recovery_slope  \\\n",
       "0   -0.43529412150382996   1034604.0393495536  -136425.28109340102   \n",
       "1     0.2704918086528778                  nan  -47073.114058666804   \n",
       "2    -1.6644736528396606    3849631.049327324   -777322.7612500505   \n",
       "3       6.08695650100708                  nan   -34836.84386688921   \n",
       "4    -1.7006802558898926   3445331.8980281185   -3997790.365641963   \n",
       "..                   ...                  ...                  ...   \n",
       "4    -0.6016949415206909                  nan   -190575.3424644765   \n",
       "5   -0.36538460850715637     3093767.41863749  -168020.42651396102   \n",
       "6     0.3083333373069763                  nan   -40818.00578898942   \n",
       "7    -0.7423312664031982   2425794.9077953044   -176238.8169412743   \n",
       "8     -0.699999988079071   1101150.1729484948  -280840.66399113426   \n",
       "\n",
       "         Sorter  \n",
       "0   sorting_KS2  \n",
       "1   sorting_KS2  \n",
       "2   sorting_KS2  \n",
       "3   sorting_KS2  \n",
       "4   sorting_KS2  \n",
       "..          ...  \n",
       "4    sorting_IC  \n",
       "5    sorting_IC  \n",
       "6    sorting_IC  \n",
       "7    sorting_IC  \n",
       "8    sorting_IC  \n",
       "\n",
       "[103 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Extracellular features, such as peak to valley duration or full-width half maximum, are important to classify neurons into putative classes. The postprocessing module of spiketoolkit allows one to compute several of these features:\n",
    "#These consist of a set of 1D features,  Metrics come from Jia et al. (2019) High-density extracellular probes reveal dendritic backpropagation and facilitate neuron classification. J Neurophys\n",
    "# https://doi.org/10.1152/jn.00680.2018\n",
    "\n",
    "#- peak to valley (peak_to_valley), time between peak and valley\n",
    "#- halfwidth (halfwidth), width of peak at half its amplitude\n",
    "#- peak trough ratio (peak_trough_ratio), amplitude of peak over amplitude of trough\n",
    "#- repolarization slope (repolarization_slope), slope between trough and return to base\n",
    "#- recovery slope (recovery_slope), slope after peak towards baseline\n",
    "# After calculation an excel database containing the concatenation of the results for all the sorters is exported in the Sorters_results folder of the recording\n",
    "\n",
    "st.postprocessing.get_template_features_list()\n",
    "Sorters = [sorting_KS2,sorting_TDC,sorting_IC]\n",
    "Sorters_name = ['sorting_KS2','sorting_TDC','sorting_IC']\n",
    "def calculate_metrics(recording_loaded,Sorters,Path_results):\n",
    "    columns_names = ['Id_key','id_cluster','peak_to_valley','halfwidth', 'peak_trough_ratio','repolarization_slope', 'recovery_slope','Sorter']\n",
    "    df=pd.DataFrame(columns = columns_names)\n",
    "    i=0\n",
    "    for s in Sorters :\n",
    "        metrics = st.postprocessing.compute_unit_template_features(recording=recording_loaded, sorting=s, as_dataframe=True)\n",
    "        metrics['id_cluster']=metrics.index\n",
    "        metrics = metrics.applymap(str)\n",
    "        metrics = metrics.assign(Sorter=Sorters_name[i])\n",
    "        metrics['Id_key'] = metrics['id_cluster']+ metrics['Sorter']\n",
    "        df = df.append(metrics)\n",
    "        i+=1\n",
    "    df.to_excel(os.path.join(Path_results,'metrics_sorters.xlsx'), index = False)\n",
    "    return df\n",
    "    \n",
    "df = calculate_metrics(recording_loaded,Sorters,Path_results)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2f6d0",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4ef82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_spikes',\n",
       " 'firing_rate',\n",
       " 'presence_ratio',\n",
       " 'isi_violation',\n",
       " 'amplitude_cutoff',\n",
       " 'snr',\n",
       " 'max_drift',\n",
       " 'cumulative_drift',\n",
       " 'silhouette_score',\n",
       " 'isolation_distance',\n",
       " 'l_ratio',\n",
       " 'd_prime',\n",
       " 'noise_overlap',\n",
       " 'nn_hit_rate',\n",
       " 'nn_miss_rate']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flavi\\anaconda3\\envs\\Spikeline\\lib\\site-packages\\spikemetrics\\metrics.py:722: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  VI = np.linalg.inv(np.cov(pcs_for_this_unit.T))\n",
      "C:\\Users\\flavi\\anaconda3\\envs\\Spikeline\\lib\\site-packages\\numpy\\lib\\function_base.py:2493: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\flavi\\anaconda3\\envs\\Spikeline\\lib\\site-packages\\numpy\\lib\\function_base.py:2493: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit 7 only has 10 spikes, which is not enough to compute metric; assigning nan...\n",
      "Unit 34 only has 10 spikes, which is not enough to compute metric; assigning nan...\n",
      "Unit 52 only has 10 spikes, which is not enough to compute metric; assigning nan...\n",
      "Unit 7 only has 10 spikes, which is not enough to compute metric; assigning nan...\n",
      "Unit 34 only has 10 spikes, which is not enough to compute metric; assigning nan...\n",
      "Unit 52 only has 10 spikes, which is not enough to compute metric; assigning nan...\n",
      "Unit 7 only has 10 spikes, which is not enough to compute metric; assigning nan...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The spiketoolkit package also provides several functions to compute qualitity metrics about the spike sorting results through the validation module\n",
    "\n",
    "#For Neuropixels recordings with minimal electrode drift, Kilosort2 performs well enough that further manual curation is not necessary. Unlike the original version of Kilosort, which required a manual merging step, Kilosort2 attempts to merge units automatically. Sometimes it over-merges, leading to units that clearly combine spikes from multiple cells. But in the majority of cases, Kilosort2 makes merging decisions as well as a human would, and does so in a way that is highly reproducible.\n",
    "\n",
    "#Because there is no \"ground truth\" information available in these datasets, any sorting algorithm is bound to make mistakes. Quality metrics allow us to understand the types of mistakes that are occurring, and obtain an estimate of their severity. Some common errors that can be identified by quality metrics include:\n",
    "#Assigning spikes from multiple neurons to the same cluster\n",
    "#Missing spikes from neurons with waveform amplitude near the spike detection threshold\n",
    "#Failing to track neurons with waveforms that change as a result of electrode drift\n",
    "#These mistakes can occur even in units that appear to be extremely well isolated. It's misleading to conceive of units as existing in two distinct categories, one with perfectly clean \"single units\" and one with impure \"multiunits.\" Instead, there's a gradient of qualities, with mostly complete, uncontaminated units at one end, and incomplete, highly contaminated units at the other.\n",
    "#List of available metrics, taken from the allen institute : https://allensdk.readthedocs.io/en/latest/_static/examples/nb/ecephys_quality_metrics.html\n",
    "st.validation.get_quality_metrics_list()\n",
    "#- num_spikes, Computes and returns the num spikes for the sorted dataset.\n",
    "#- firing_rate, Firing rate is equal to the total number of spikes divided by the number of seconds in the recording.\n",
    "#- presence_ratios, It measures the fraction of time during a session in which a unit is spiking, and ranges from 0 to 0.99 (an off-by-one error in the calculation ensures that it will never reach 1.0).\n",
    "#- isi_violations, Inter-spike-interval (ISI) violations are a classic measure of unit contamination. Because all neurons have a biophysical refractory period, we can assume that any spikes occurring in rapid succession (<1.5 ms intervals) come from two different neurons. Therefore, the more a unit is contaminated by spikes from multiple neurons, the higher its isi_violations value will be.\n",
    "#- amplitude cutoff, Unlike presence ratio, which detects units that drift out of the recording, amplitude cutoff provides an estimate of the false negative rate—e.g., the fraction of spikes below the spike detection threshold. Thus, amplitude cutoff is a measure of unit \"completeness\" that is complementary to presence ratio.\n",
    "#- isolation_distances,Isolation distance calculates the size of the 96-dimensional sphere that includes as many \"other\" spikes as are contained in the original unit's cluster, after normalizing the clusters by their standard deviation in each dimension (Mahalanobis distance).\n",
    "#- nn_metrics, This metric looks at the PCs for one unit and calculates the fraction of their nearest neighbors that fall within the same cluster. If a unit is highly contaminated, then many of the closest spikes will come from other units\n",
    "#- These additional metrics are added to the previous database with the postprocessing metrics\n",
    "def calculate_additional_metrics(recording_loaded,Sorters,Path_results):\n",
    "    df = pd.read_excel(os.path.join(Path_results,'metrics_sorters.xlsx'))\n",
    "    columns_names_to_add =  [\"Id_key\",'id_cluster',\"num_spikes\", \"firing_rate\", \"presence_ratio\", \"isi_violation\", \"amplitude_cutoff\", \"snr\",\"max_drift\", \"cumulative_drift\", \"silhouette_score\", \"isolation_distance\", \"l_ratio\",\"d_prime\", \"nn_hit_rate\", \"nn_miss_rate\"]\n",
    "    df2=pd.DataFrame(columns = columns_names_to_add)\n",
    "    i=0\n",
    "    Metrics_per_sorter = list()\n",
    "    for s in Sorters :\n",
    "        metrics2 = st.validation.compute_quality_metrics(sorting=s, recording=recording_loaded, metric_names=[\"num_spikes\", \"firing_rate\", \"presence_ratio\", \"isi_violation\", \"amplitude_cutoff\", \"snr\",\"max_drift\", \"cumulative_drift\", \"silhouette_score\", \"isolation_distance\", \"l_ratio\",\"d_prime\", \"nn_hit_rate\", \"nn_miss_rate\"], as_dataframe=True)\n",
    "        name = [metrics2,Sorters_name[i]]\n",
    "        Metrics_per_sorter.append(name)\n",
    "        metrics2['id_cluster']=metrics2.index\n",
    "        metrics2 = metrics2.applymap(str)\n",
    "        metrics2 = metrics2.assign(Sorter=Sorters_name[i])\n",
    "        metrics2['Id_key'] = metrics2['id_cluster']+ metrics2['Sorter']\n",
    "        df2 = df2.append(metrics2)\n",
    "        i+=1\n",
    "    df_allmetrics = pd.merge(df, df2, on=\"Id_key\")\n",
    "    df_allmetrics.to_excel(os.path.join(Path_results,'all_metrics_sorters.xlsx'), index = False)\n",
    "    return df_allmetrics, Metrics_per_sorter\n",
    "\n",
    "\n",
    "df_allmetrics,Metrics_per_sorter = calculate_additional_metrics(recording_loaded,Sorters,Path_results)\n",
    "display(df_allmetrics)\n",
    "\n",
    "#Some unit metrics couldn't be calculated because of the low number of spikes\n",
    "print(Metrics_per_sorter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba0c5a",
   "metadata": {},
   "source": [
    "## Curation - Quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92980b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "The curation can be done by manual curation through Phy and/or automatic curation through the quality metrics and/or consensus based curation\n",
    "For the purpose of this end to end example the results quality metrics of Kilosort2  will be calculated and plotted in order to obtain a dataframe containing the id clusters corresponding to the clusters to check manually on PHy since they crossed a given threshold, for example having a presence-ratio of less than 90% of the other units\n",
    "By analyzing the dataframe obtained previously in the excel file we can observe that 83 units have been found by Kilosort2, 8 by Ironclust and 14 by Tridesclous\n",
    "After spike sorting and computing validation metrics, you can automatically curate the spike sorting output using the quality metrics. This can be done with the toolkit.curation submodule.\n",
    "Let's look in more detail at the distribution of some quality metrics across 83 units. We'll start by creating a function for plotting each metric in an aesthetically pleasing way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba905620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Firing_rate plotting example\n",
    "def plot_metric(data, bins, x_axis_label, color, max_value=-1):\n",
    "    \n",
    "    h, b = np.histogram(data, bins=bins, density=True)\n",
    "    \n",
    "    x = b[:-1]\n",
    "    y = gaussian_filter1d(h, 1)\n",
    "    plt.plot(x, y, color=color)\n",
    "    plt.xlabel(x_axis_label)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "    [plt.gca().spines[loc].set_visible(False) for loc in ['right', 'top', 'left']]\n",
    "    if max_value < np.max(y) * 1.1:\n",
    "        max_value = np.max(y) * 1.1\n",
    "    plt.ylim([0, max_value])\n",
    "    \n",
    "    return max_value,h,b\n",
    "\n",
    "Dataframe_KS_metrics = Metrics_per_sorter[0][0]\n",
    "\n",
    "data_firingrate_KSresults = Dataframe_KS_metrics['firing_rate']\n",
    "bins1 = np.linspace(0,50,100)\n",
    "firing_rate_plot = plot_metric(data_firingrate_KSresults, bins1, 'Firing rate (Hz)', 'red')\n",
    "\n",
    "#Since there are many units with low firing rates, let's use a log scale instead\n",
    "\n",
    "data_firingrate_KSresults_log = np.log10(Dataframe_KS_metrics['firing_rate'])\n",
    "bins2 = np.linspace(-3,2,100)\n",
    "firing_rate_log, h, b = plot_metric(data_firingrate_KSresults_log, bins2, 'log$_{10}$ firing rate (Hz)', 'red')\n",
    "\n",
    "# We can see that 90% of units that fire in the 7 Hz range after transformation of the log scale\n",
    "            \n",
    "def table_clusters_to_analyze(Dataframe_KS_metrics):\n",
    "    Metrics = [\"firing_rate\", \"presence_ratio\", \"isi_violation\", \"amplitude_cutoff\", \"nn_hit_rate\"]\n",
    "    Columns = ['Id_cluster'] + Metrics\n",
    "    df = pd.DataFrame(columns = Columns)\n",
    "    df['Id_cluster']= Dataframe_KS_metrics.index\n",
    "    df.set_index('Id_cluster',drop=False,inplace=True)\n",
    "    firing_rate_threshold = np.percentile(Dataframe_KS_metrics['firing_rate'],90)\n",
    "    #All the clusters having a firing_rate less than 90% of the other clusters firing rate accross all the clusters will be highlighted\n",
    "    presence_ratio_threshold = np.percentile(Dataframe_KS_metrics['presence_ratio'],50)\n",
    "    #All the clusters having a presence_ratio less than the median presence ratio accross all the clusters will be highlighted\n",
    "    isi_violation_threshold = 0.5\n",
    "    #All the clusters having a  isi_violation higher than 0.5 will be highlighted for further analysis, 0.5 means that contamining spikes are occurring at roughly half the rate of \"true\" spikes for that unit. \n",
    "    amplitude_cutoff_threshold= 0.3\n",
    "    #All the clusters having an amplitude cutoff higher than 0.3, meaning that 30% of the spikes are missing from the unit, it is the level of 'False negatives' and is completing the presence ratio analysis\n",
    "    nn_hit_rate_threshold = 0.1\n",
    "    #All the clusters having a nearest neighboor hit rate bigger than 10%, meaning that the fraction of their nearest neighbors that fall within the same cluster is higher than 10%\n",
    "    Metrics=[[\"firing_rate\",firing_rate_threshold],['presence_ratio',presence_ratio_threshold], [\"isi_violation\",isi_violation_threshold], [\"amplitude_cutoff\",amplitude_cutoff_threshold],  [\"nn_hit_rate\",nn_hit_rate_threshold]]\n",
    "    #Thresholds = [firing_rate_threshold,presence_ratio_threshold,isi_violation_threshold,amplitude_cutoff_threshold,isolation_distance_threshold,nn_hit_rate_threshold]\n",
    "    for i in Dataframe_KS_metrics.index:\n",
    "        for m in Metrics :\n",
    "            if m[0]==\"presence_ratio\":\n",
    "                if Dataframe_KS_metrics.loc[i][m[0]]< m[1]:\n",
    "                    c = Dataframe_KS_metrics.loc[i][m[0]] \n",
    "                    df.at[[i],[m[0]]]=c\n",
    "            else:\n",
    "                if Dataframe_KS_metrics.loc[i][m[0]] > m[1]:\n",
    "                    c = Dataframe_KS_metrics.loc[i][m[0]] \n",
    "                    df.at[[i],[m[0]]]=c\n",
    "    df.to_excel(os.path.join(Path_results,'clusters_to_analyze_manually.xlsx'), index = False)\n",
    "    return df\n",
    "\n",
    "df = table_clusters_to_analyze(Dataframe_KS_metrics)\n",
    "#The following table gives the ID clusters that need to be analyzed further manually in Phy since they are crossing the thresholds settled concerning the quality metrics\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b229807",
   "metadata": {},
   "source": [
    "## Curation - Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d9289",
   "metadata": {},
   "source": [
    "\n",
    "We can now open the Phy view of the Kilosort2 results to start the manual aspect of the analysis based on the insights of the quality metrics.\n",
    "\n",
    "The next step is then to go through this data and quickly verify that the sorting was efficient and coherent. \n",
    "\n",
    "In the Phy2 interface, we have access to all the data Kilosort2 was able to extract from the raw electrophysiological feed we provided it with, and we can manually check the treatment to fine tune the spike sorting.\n",
    "\n",
    "To do so we integrate the insights obtained thanks to the table 'clusters to analyze' previously obtained by calculating the quality metrics.\n",
    "\n",
    "The first step in this checking process is to look at the spike template window to see if the spikes were detected on the same channels and if they have similar shapes. If so, we can look at a principal component decomposition of the templates and compare them to see if they are similar along with the analysis of the similarity rate given by Phy2.\n",
    "\n",
    "In the case where the components do match, we have a final look the crosscorrelogram to check on the refractory period. If the refractory period is not violated, we can merge the two clusters to create a single new one containing all of the spikes from the previous clusters. If at any point of this analysis we see a cluster that seems to have two distinct clusters in it (violation of the refractory period in the auto-correlogram and/or clear distinction of two groups in the principal component analysis), we can select the spikes we think belong to a separate group and split that cluster into two, one containing the selected spikes and the other containing the remaining spikes. After deciding to split or merge a cluster, we can label it based on our observation and criteriafor selection: “good” means we think the spikes have likely been emitted by a single neuron and that this cluster will be useful in our analysis; “multi-unit activity”, or “mua”, if we think the spikes have likely been produced by distinct neurons but are too noisy to be told apart, while still being useful for global statistics and computations; or “noise”, when we consider that the signal recorded is an artifact or does not contribute anything useful to our analysis of the data, so we want to exclude it from further computations and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Before the manual curation through Phy2 helped with the quality metrics table we have 79 units\n",
    "print(len(sorting_KS2.get_unit_ids()))\n",
    "\n",
    "\n",
    "#Below can be seen the waveform view from Phy2, corresponding to the typical shape of a spike qualified as 'good'\n",
    "#After curating the results we can reload it using the PhySortingExtractor, we exclude the cluster groups corresponding to noise\n",
    "Phy_path = r'F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\Sorting_results\\_KS2_results\\Phy'\n",
    "sorting_KS_phy_curated = se.PhySortingExtractor(Phy_path, exclude_cluster_groups=['noise'])\n",
    "\n",
    "#After curation only 19 units are left\n",
    "print(len(sorting_KS_phy_curated.get_unit_ids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d98b8a",
   "metadata": {},
   "source": [
    "## Curation - Consensus based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86a78f",
   "metadata": {},
   "source": [
    "\n",
    "We can now do the last type of curation which is the agreement sorting based on our half-automatic curation of the Kilosort2 results and the results of Tridesclous along with Ironclust\n",
    "We can use the comparison module. We first compare and match the output spike trains of the different sorters, and we can then extract a new SortingExtractor with only the units in agreement.\n",
    "When extracting the units in agreement, the spike trains are modified so that only the true positive spikes between the comparison with the best match are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b12d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sorters2=[sorting_KS_phy_curated,sorting_TDC,sorting_IC]\n",
    "mcmp = sc.compare_multiple_sorters(Sorters2, ['KS2curated', 'TDC','IC'])\n",
    "#The total number of units for which k sorters agree (unit agreement is defined as 50% spike match).\n",
    "w = sw.plot_multicomp_agreement(mcmp)\n",
    "#The number of units (per sorter) for which k sorters agree; most sorters find many units that other sorters do not.\n",
    "w = sw.plot_multicomp_agreement_by_sorter(mcmp)\n",
    "\n",
    "agreement_sorting = mcmp.get_agreement_sorting(minimum_agreement_count=2)\n",
    "isinstance(agreement_sorting, se.SortingExtractor)\n",
    "print(len(agreement_sorting.get_unit_ids()))\n",
    "\n",
    "Phy_agreement_folder = r'F:\\SO8002\\Desktop\\Data_presentation\\003WO.1_007_PVTmanual_slowslow_g0_imec0\\Sorting_results\\Phy-agr'\n",
    "st.postprocessing.export_to_phy(recording_loaded, \n",
    "                                agreement_sorting, output_folder=Phy_agreement_folder,verbose=True, recompute_info=True, max_channels_per_template=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842c2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Spikeline",
   "language": "python",
   "name": "spikeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
